---
title: "Assumptions Part 1: Normality"
description: "A discussion of the assumption of normality"
author:
  - name: Andy Field
    url: https://profandyfield.com
date: 06 08 2012
categories: [Statistics, OLS linear models, The assumption of normality] # self-defined categories
citation: 
  url: https://profandyfield.com/posts/2012_08_06_normality/ 
image: qq.png
draft: false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!
---

<br /><span style="font-family:Arial, Helvetica, sans-serif;">.... I didn't grow a pair of breasts. If you didn't read my last blog that comment won't make sense, but it turns out that people like breasts so I thought I'd mention them again. I haven't written a lot of blogs, but my frivolous blog about growing breasts as a side effect of some pills was (by quite a large margin) my most viewed blog. It's also the one that took me the least time to write and that I put the least thought into. I think the causal factor might be the breasts.</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><span style="font-family:Arial, Helvetica, sans-serif;">This blog isn't about breasts, it's about normality. Admittedly the normal distribution looks a bit like a nipple-less breast, but it's not one: I'm very happy that my wife does not sport two normal distributions upon her lovely chest. I like stats, but not that much ...</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><br /><h3><span style="font-family:Arial, Helvetica, sans-serif;">Assumptions</span></h3><br /><span style="font-family:Arial, Helvetica, sans-serif;">Anyway, I recently stumbled across <a href="http://www.frontiersin.org/quantitative_psychology_and_measurement/10.3389/fpsyg.2012.00137/abstract?utm_source=newsletter&amp;utm_medium=email&amp;utm_campaign=Psychology-w28-2012"><span id="goog_2046660709"></span>this paper<span id="goog_2046660710"></span></a>. The authors sent a sample of postgrads (with at least 2 years research experience) a bunch of data analysis scenarios and asked them how they would analyze the data. They were interested in whether or not, and how these people checked the assumptions of the tests they chose to use. The good news was that they chose the correct test (although given all of the scenarios basically required a general linear model of some variety that wasn’t hard). However, not many of them checked assumptions. The conclusion as that people don’t understand assumptions or how to test them</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><span style="font-family:Arial, Helvetica, sans-serif;">I get asked about assumptions a lot. I also have to admit to hating the chapter on assumptions in my SPSS and R books. Well, hate is a strong word, but I think it toes a very conservative and traditional line. In my recent update of the SPSS book (out early next year before you ask) I completely re-wrote this chapter. It takes a very different approach to thinking about assumptions.</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><span style="font-family:Arial, Helvetica, sans-serif;">Most of the models we fit to data sets are based on the general linear model, (GLM) which means that any assumption that applies to the GLM (i.e., regression) applies to virtually everything else. You don’t really need to memorize a list of different assumptions for different tests: if it’s a GLM (e.g., ANOVA, regression etc.) then you need to think about the assumptions of regression. The most important ones are:</span><br /><br /><ul><li><span style="font-family:Arial, Helvetica, sans-serif;">Linearity</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;">Normality (of residuals)</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;">Homoscedasticity (aka homogeneity of variance)</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;">Independence of errors.</span></li></ul><br /><h3><span style="font-family:Arial, Helvetica, sans-serif;">What Does Normality Affect?</span></h3><span style="font-family:Arial, Helvetica, sans-serif;">For this post I’ll discuss normality. If you’re thinking about normality, then you need to think about 3 things that rely on normality:</span><br /><br /><ol><li><span style="font-family:Arial, Helvetica, sans-serif;"><b>Parameter estimates</b>: That could be an estimate of the mean, or a <i>b</i> in regression (and a <i>b</i> in regression can represent differences between means). Models have error (i.e., residuals), and if these residuals are normally distributed in the population then using the method of least squares to estimate the parameters (the <i>b</i>s) will produce better estimates than other methods.</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;"><b>Confidence intervals</b>: whenever you have a parameter, you usually want to compute a confidence interval (CI) because it’ll give you some idea of what the population value of the parameter is. We use values of the standard normal distribution to compute the confidence interval: using values of the standard normal distribution makes sense only if the parameter estimates actually come from one.</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;"><b>Significance tests</b>: we often test parameters against a null value (usually we’re testing whether <i>b</i> is different from 0). For this process to work, we assume that the parameter estimates have a normal distribution. We assume this because the test statistics that we use (such as the <i>t</i>, <i>F</i> and chi-square), have distributions related to the normal. If parameter estimates don’t have a normal distribution then <i>p</i>-values won’t be accurate. </span></li></ol><br /><h3><span style="font-family:Arial, Helvetica, sans-serif;">What Does The Assumption Mean?</span></h3><br /><span style="font-family:Arial, Helvetica, sans-serif;">People often think that your data need to be normally distributed, and that’s what many people test. However, that’s not the case. What matters is that the residuals in the population are normal, and the sampling distribution of parameters is normal. However, we don’t have access to the sampling distribution of parameters or population residuals; therefore, we have to guess at what might be going on by testing the data instead.</span><br /><br /><h3><span style="font-family:Arial, Helvetica, sans-serif;">When Does The Assumption Matter?</span></h3><br /><span style="font-family:Arial, Helvetica, sans-serif;">However, the central limit theorem tells us that no matter what distribution things have, the sampling distribution will be normal if the sample is large enough. How large is large enough is another matter entirely and depends a bit on what test statistic you want to use. So bear that in mind. However, oversimplifying things a bit, we could say:</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><br /><ol><li><b style="font-family:Arial, Helvetica, sans-serif;">Confidence intervals</b><span style="font-family:Arial, Helvetica, sans-serif;">: For confidence intervals around a parameter estimate to be accurate, that estimate must come from a normal distribution. The central limit theorem tells us that in large samples, the estimate will have come from a normal distribution regardless of what the sample or population data look like. Therefore, if we are interested in computing confidence intervals then we don’t need to worry about the assumption of normality if our sample is large enough. (There is still the question of how large is large enough though.) You can easily construct bootstrap confidence intervals these days, so if your interest is confidence intervals then why not stop worrying about normality and use bootstrapping instead?</span></li><li><span style="font-family:Arial, Helvetica, sans-serif;"><b>Significance tests</b>: For significance tests of models to be accurate the sampling distribution of what’s being tested must be normal. Again, the central limit theorem tells us that in large samples this will be true no matter what the shape of the population. Therefore, the shape of our data shouldn’t affect significance tests provided our sample is large enough. (How large is large enough depends on the test statistic and the type of non-normality. Kurtosis for example tends to screw things up quite a bit.) You can make a similar argument for using bootstrapping to get a robust <i>p </i>if p<i> </i>is your thing<i>.</i></span></li><li><span style="font-family:Arial, Helvetica, sans-serif;"><b>Parameter Estimates</b>: The method of least squares will always give you an estimate of the model parameters that minimizes error, so in that sense you don’t need to assume normality of anything to fit a linear model and estimate the parameters that define it (Gelman &amp; Hill, 2007). However, there are other methods for estimating model parameters, and if you happen to have normally distributed errors then the estimates that you obtained using the method of least squares will have less error than the estimates you would have got using any of these other methods. </span></li></ol><br /><h3><span style="font-family:Arial, Helvetica, sans-serif;">Summary</span></h3><span style="font-family:Arial, Helvetica, sans-serif;">If all you want to do is estimate the parameters of your model then normality doesn’t really matter. If you want to construct confidence intervals around those parameters, or compute significance tests relating to those parameters then the assumption of normality matters in small samples, but because of the central limit theorem we don’t really need to worry about this assumption in larger samples. The question of how large is large enough is a complex issue, but at least you know now what parts of your analysis will go screwy if the normality assumption is broken..</span><br /><span style="font-family:Arial, Helvetica, sans-serif;"><br /></span><span style="color:#990000;font-family:Arial, Helvetica, sans-serif;"><b>This blog is based on excerpts from the forthcoming 4th edition of ‘Discovering Statistics Using SPSS: and sex and drugs and rock ‘n’ roll’.</b></span><br /><div><br /></div>
